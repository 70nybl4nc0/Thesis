\chapter{Propuesta}\label{chapter:proposal}

UTHOPIA propone un modelo de evaluación que sirve para comparar la inteligencia de humanos con la inteligencia de agentes de aprendizaje por refuerzo utilizando como base las premisas planteadas por (\cite{chollet2019measure}) en el ARC. Brinda una interfaz para crear juegos que se centren en diferentes tareas que requieran habilidades carácter cognitivo, cada una de las cuales pone a prueba un aspecto diferente del razonamiento. Aunque los supuestos prejuicios que planteamos son sencillos, al combinarlos podemos crear muchos problemas complejos que requieren la expresión de razonamiento cognitivo similar al humano.

Las carácteristicas que se buscaron al construir UTHOPIA fueron:

\begin{itemize}
    \item Integrar evaluaciones tanto para humanos como para agentes con algoritmos de aprendizaje por reforzamiento.
    \item Centrarse en la medición de la generalización consciente del desarrollador (\ref{section:state-of-the-art:generalization-on-machine-learning}) en lugar de la habilidad específica, presentando únicamente tareas novedosas que se supone son desconocidas para el desarrollador del sistema.
    \item Medir de forma cualitativamente amplia la generalización, presentando tareas abstractas que deben ser comprendidas por el examinado con pocos ejemplos o tiempo para probar.
    \item Describir explícitamente el conjunto completo de prejuicios con los que cuenta y permitir una comparación de inteligencia general justa entre humanos y máquinas al requerir únicamente prejuicios cercanos al conocimiento previo humano innato.
\end{itemize}

\section{Prejuicios del conocimiento} 

Cualquier prueba de inteligencia va a implicar conocimientos previos. ARC trata de controlar sus propias suposiciones enumerando explícitamente el conocimiento previo que asume y evitando depender de cualquier información que no sea parte de este conocimiento previo (por ejemplo, el conocimiento adquirido como el lenguaje). Esto permite que el programa sea lo más cercano posible a los conocimientos previos innatos humanos, proporcionando un terreno justo para comparar la inteligencia artificial y la inteligencia humana, como recomendamos en (\ref{section:state-of-the-art:a-good-measure-of-inteligence:human-prios})

\textbf{Premisas de objetividad}
\begin{itemize}
\item Cohesión y delimitación de objetos: Definir las áreas que representan un objeto.
\item Persistencia de los objetos: Los objetos tienden a persistir en el juego al menos que se realice algun tipo de interacción explícita.
\item Influencia de los objetos a través del contacto: La mayoria de acciones se realizan mediante el contacto entre objetos.
\item Movimiento y dirección explícitos: La dirección y la movilidad de los objetos es deducible a partir de sus cambios de su posición en el tiempo.
\end{itemize}

\textbf{Números y capacidades de conteo}
\begin{itemize}
    \item Conteo de objetos ya sea que cumplen con similaridad o determinadas propiedades dadas por las tareas.
\end{itemize}

\textbf{Conocimientos básicas de geometría y topología}
\begin{itemize}
\item Puntos, lineas y formas.
\item Similaridad entre objetos que por su color y forma se asocian al mismo tipo de función.
\item Simetrías, rotaciones y traslaciones.
\item Aumento o disminución de la forma, distorsiones de escala.
\item Contener, ser contenido y estar dentro o fuera de un perímetro.
\item Alcanzar o permanecer en el borde de un perimetro.
\item Superposición de objetos.
\end{itemize}

Como se ha sugerido en (\ref{section:state-of-the-art:a-good-measure-of-inteligence:human-prios}), no se incluye señales o simbolos cuya interpretación requiera conocimientos adquiridos tales como el lenguaje, numeros, señalamientos entre otros. 

\section{Estructura de UTHOPIA}

\subsection{Características de los juegos}

Los juegos de UTHOPIA son simples. El objetivo de cada uno es capturar elementos del razonamiento cognitivo. Se intenta retirar todos los factores distractores que no tengan peso en el significado abstracto de la tarea evaluada. Cada juego esta formado por objetos de color entero y con formas simples. Su interacciones ocurre mediante el contacto entre ellos. No es necesario jugar de forma optima para la terminar los juegos correctamente, y en su mayoria la dificultad es ajustable además que se ha intentado que todos los juegos tengan solución en cada episodio. 

Para brindar experiencias que no lleven a los agentes a sobre ajustarse se ejecuta en cada episodio de juego, la regeneracion del estado inicial de forma procedural.

\subsection{Configuración del aprendizaje por refuerzo}

Desde la perspectiva del aprendizaje por refuerzo, para un solo agente, definimos las observaciones, las acciones y las recompensas de la siguiente manera (véase el material complementario para más detalles):

\begin{itemize}
    \item Observaciones: Una única cámara que muestra una cuadrícula de píxeles con resolución $K * K * 3$ donde $32 \eqslantless K \eqslantless 256$.
    \item Espacio de acción: El agente puede tomar hasta $8$ acciones, de ellas las primeras $4$ indican dirección de forma semantica, cuyo efecto concreto dentro del juego puede variar según se diseñe. Tanto las acciones de dirección como las restantes pueden ser o no válidas para todos los juegos, por eso en cada uno se especificará explicitamente las acciones aceptadas.
    \item Señal de Recompensa: Cada juego tiene una evaluación binaria. Las recomenzas están dadas por la terminación correcta o incorrecta de los mismos. Cada juego tiene asociado una cantidad maxima de tiempo de juego, luego de este el juego termina fallido.
\end{itemize} 


\subsection{El flujo de evaluación para agentes inteligentes en UTHOPIA}

La evaluación de los agentes se produce en dos etapas: Preparación y Evaluación.

Durante la \textbf{Etapa de Preparación} UTHOPIA selecciona un juego al azar y crea continuamente instancias de este para que el agente se familiarice y capte la esencia detras de la tarea cognitiva. Se expone al juego durante un tiempo determinado relativamente corto (comparado al entrenamiento habitual de los algoritmos de aprendizaje por reforzamiento) algo mayor al que un humano promedio necesitaria para entender la tarea en cuestion para forzarlo a ser eficiente. 

Luego de esto se procede a la \textbf{Etapa de Prueba} donde se crean nuevas instancias del juego seleccionado pero, esta vez, evaluando la solución efectiva del mismo. La conclución de la evaluación es binaria basandose en la solucion o no del juego en un numero suficiente de intentos.

Una Preparar al agente solo con un Juego especifico, y apartir de ahi controlar explicitamente el orden de los juegos a los que lo someteremos, segun las premisas que se utilizan, y midiendo su capacidad de resolverlos partiendo de la dificultad de generalizacion del juego inicial. Esto nos permitiría medir su capacidad de generalización de forma mas precisa.

La idea es:

1. Uthopia selecciona un juego de la base de juegos
2. Comienza la fase de Preparación
3. Dinamica de Generar instancia  de Juego < - > Realizar la simulacion
4. Pasado un tiempo t, se comienza la fase de Prueba
5. Lo mismo que en 3. Pero al terminar el episodio se revisan los resultados.
6. Luego de X intentos, suficientes, se retornan los resultados a Uthopia para anializar. 
7. Si así se decide (pasa la prueba), comienza el paso 1 otra vez.