@article{chollet2019measure,
  title   = {On the measure of intelligence},
  author  = {Chollet, Fran{\c{c}}ois},
  journal = {arXiv preprint arXiv:1911.01547},
  year    = {2019}
}

@inproceedings{10.1145/3311350.3357716,
  author    = {Hofmann, Katja},
  title     = {Minecraft as AI Playground and Laboratory},
  year      = {2019},
  isbn      = {9781450366885},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3311350.3357716},
  doi       = {10.1145/3311350.3357716},
  abstract  = {Modern video games provide exciting challenges and opportunities for pushing the state of the art in machine learning and other research areas, and, in turn, stand to first benefit from research advances. For driving research, games provide rich data that can be used to tackle hard problems, from complex decision making to collaboration. If and when these are successfully tackled, new algorithms and insights have the potential to enable entirely new game experiences.This talk focuses on opportunities in the setting of the game Minecraft, one of the most popular video games of all time. Minecraft is an open-world game, where players explore, create, and continuously find new ways to play and engage with each other. This open-ended nature both make the game appealing to its human fan-base, and uniquely challenging to AI algorithms. To unlock the potential of Minecraft for AI experimentation, my team has developed Project Malmo -- an open source experimentation platform built on top of Minecraft to enable a wide range of research. Here, I will illustrate the capabilities of the platform with recent examples that I find particularly exciting.I will highlight our most recent collaboration, led by a team of PhD students at Carnegie Mellon University: the MineRL competition. This ambitious competition is designed to drive advances in sample efficient reinforcement learning with human priors. Sample efficient learning is a key challenge, with current algorithms often requiring millions of samples to learn to perform individual narrow tasks, limiting the scope and applicability of these approaches. This competition is built around a complex task, large-scale demonstration data, and an evaluation setup that requires and rewards sample efficient learning and effective generalization.Looking out into the future, I will conclude by highlight selected open questions and challenges that have high potential for impact in video games and raise key questions for current state-of-the-art AI approaches.},
  booktitle = {Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
  pages     = {1},
  numpages  = {1},
  keywords  = {video games, engagement, machine learning, minecraft, minerl, project malmo},
  location  = {Barcelona, Spain},
  series    = {CHI PLAY '19}
}

@inproceedings{guss2019the,
  author    = {Guss, William H and Codel, Cayden and Hofmann, Katja and Houghton, Brandon and Kuno, Noboru Sean and Milani, Stephanie and Mohanty, Sharada and Liebana, Diego Perez and Salakhutdinov, Ruslan and Topin, Nicholay and Veloso, Manuela and Wang, Philip},
  title     = {The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors},
  booktitle = {Thirty-third Conference on Neural Information Processing Systems (NeurIPS) Competition track},
  year      = {2019},
  month     = {December},
  abstract  = {Though deep reinforcement learning has led to breakthroughs in many difficult domains, these successes have required an ever-increasing number of samples. As state-of-the-art reinforcement learning (RL) systems require an exponentially increasing number of samples, their development is restricted to a continually shrinking segment of the AI community. Likewise, many of these systems cannot be applied to real-world problems, where environment samples are expensive. Resolution of these limitations requires new, sample-efficient methods. To facilitate research in this direction, we introduce the MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors.
               
               The primary goal of the competition is to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments. To that end, we introduce:(1) the Minecraft ObtainDiamond task, a sequential decision making environment requiring long-term planning, hierarchical control, and efficient exploration methods; and (2) the MineRL-v0 dataset, a large-scale collection of over 60 million state-action pairs of human demonstrations that can be resimulated into embodied trajectories with arbitrary modifications to game state and visuals.},
  url       = {https://www.microsoft.com/en-us/research/publication/the-minerl-competition-on-sample-efficient-reinforcement-learning-using-human-priors/}
}

@article{wang2018glue,
  title   = {GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author  = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal = {arXiv preprint arXiv:1804.07461},
  year    = {2018}
}

@article{bellemare2013arcade,
  title   = {The arcade learning environment: An evaluation platform for general agents},
  author  = {Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {47},
  pages   = {253--279},
  year    = {2013}
}

@article{mccarthy1987generality,
  title     = {Generality in artificial intelligence},
  author    = {McCarthy, John},
  journal   = {Communications of the ACM},
  volume    = {30},
  number    = {12},
  pages     = {1030--1035},
  year      = {1987},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{yampolskiy2021differences,
  title     = {On the Differences between Human and Machine Intelligence.},
  author    = {Yampolskiy, Roman},
  booktitle = {AISafety@ IJCAI},
  year      = {2021}
}

@article{hernandez2020twenty,
  title     = {Twenty years beyond the turing test: Moving beyond the human judges too},
  author    = {Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal   = {Minds and Machines},
  volume    = {30},
  number    = {4},
  pages     = {533--562},
  year      = {2020},
  publisher = {Springer}
}

@inproceedings{cobbe2020leveraging,
  title        = {Leveraging procedural generation to benchmark reinforcement learning},
  author       = {Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John},
  booktitle    = {International conference on machine learning},
  pages        = {2048--2056},
  year         = {2020},
  organization = {PMLR}
}

@article{hernandez2017evaluation,
  title     = {Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement},
  author    = {Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal   = {Artificial Intelligence Review},
  volume    = {48},
  number    = {3},
  pages     = {397--447},
  year      = {2017},
  publisher = {Springer}
}

@article{samvelyan2021minihack,
  title   = {Minihack the planet: A sandbox for open-ended reinforcement learning research},
  author  = {Samvelyan, Mikayel and Kirk, Robert and Kurin, Vitaly and Parker-Holder, Jack and Jiang, Minqi and Hambro, Eric and Petroni, Fabio and K{\"u}ttler, Heinrich and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal = {arXiv preprint arXiv:2109.13202},
  year    = {2021}
}

@article{kuttler2020nethack,
  title   = {The nethack learning environment},
  author  = {K{\"u}ttler, Heinrich and Nardelli, Nantas and Miller, Alexander and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {7671--7684},
  year    = {2020}
}

@article{juliani2019obstacle,
  title   = {Obstacle tower: A generalization challenge in vision, control, and planning},
  author  = {Juliani, Arthur and Khalifa, Ahmed and Berges, Vincent-Pierre and Harper, Jonathan and Teng, Ervin and Henry, Hunter and Crespi, Adam and Togelius, Julian and Lange, Danny},
  journal = {arXiv preprint arXiv:1902.01378},
  year    = {2019}
}

@article{justesen2018illuminating,
  title   = {Illuminating generalization in deep reinforcement learning through procedural level generation},
  author  = {Justesen, Niels and Torrado, Ruben Rodriguez and Bontrager, Philip and Khalifa, Ahmed and Togelius, Julian and Risi, Sebastian},
  journal = {arXiv preprint arXiv:1806.10729},
  year    = {2018}
}

@article{nichol2018gotta,
  title   = {Gotta learn fast: A new benchmark for generalization in rl},
  author  = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
  journal = {arXiv preprint arXiv:1804.03720},
  year    = {2018}
}

@inproceedings{kanagawa2019rogue,
  title        = {Rogue-gym: A new challenge for generalization in reinforcement learning},
  author       = {Kanagawa, Yuji and Kaneko, Tomoyuki},
  booktitle    = {2019 IEEE Conference on Games (CoG)},
  pages        = {1--8},
  year         = {2019},
  organization = {IEEE}
}

@inproceedings{balla2020evaluating,
  title        = {Evaluating generalisation in general video game playing},
  author       = {Balla, Martin and Lucas, Simon M and Perez-Liebana, Diego},
  booktitle    = {2020 IEEE Conference on Games (CoG)},
  pages        = {423--430},
  year         = {2020},
  organization = {IEEE}
}

@inproceedings{cobbe2019quantifying,
  title        = {Quantifying generalization in reinforcement learning},
  author       = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1282--1289},
  year         = {2019},
  organization = {PMLR}
}

@article{chen2020reinforcement,
  title   = {Reinforcement learning generalization with surprise minimization},
  author  = {Chen, Jerry Zikun},
  journal = {arXiv preprint arXiv:2004.12399},
  year    = {2020}
}

@article{alver2020brief,
  title   = {A brief look at generalization in visual meta-reinforcement learning},
  author  = {Alver, Safa and Precup, Doina},
  journal = {arXiv preprint arXiv:2006.07262},
  year    = {2020}
}

@article{azad2021scenic4rl,
  title   = {Scenic4RL: programmatic modeling and generation of reinforcement learning environments},
  author  = {Azad, Abdus Salam and Kim, Edward and Wu, Qiancheng and Lee, Kimin and Stoica, Ion and Abbeel, Pieter and Seshia, Sanjit A},
  journal = {arXiv preprint arXiv:2106.10365},
  year    = {2021}
}

@article{raileanu2020ride,
  title   = {Ride: Rewarding impact-driven exploration for procedurally-generated environments},
  author  = {Raileanu, Roberta and Rockt{\"a}schel, Tim},
  journal = {arXiv preprint arXiv:2002.12292},
  year    = {2020}
}

@inproceedings{tomilin2022levdoom,
  title        = {LevDoom: A Benchmark for Generalization on Level Difficulty in Reinforcement Learning},
  author       = {Tomilin, Tristan and Dai, Tianhong and Fang, Meng and Pechenizkiy, Mykola},
  booktitle    = {2022 IEEE Conference on Games (CoG)},
  pages        = {72--79},
  year         = {2022},
  organization = {IEEE}
}

@inproceedings{raileanu2021decoupling,
  title        = {Decoupling value and policy for generalization in reinforcement learning},
  author       = {Raileanu, Roberta and Fergus, Rob},
  booktitle    = {International Conference on Machine Learning},
  pages        = {8787--8798},
  year         = {2021},
  organization = {PMLR}
}

@article{hafner2021benchmarking,
  title   = {Benchmarking the spectrum of agent capabilities},
  author  = {Hafner, Danijar},
  journal = {arXiv preprint arXiv:2109.06780},
  year    = {2021}
}

@inproceedings{hofmann2019minecraft,
  title     = {Minecraft as ai playground and laboratory},
  author    = {Hofmann, Katja},
  booktitle = {Proceedings of the annual symposium on computer-human interaction in play},
  pages     = {1--1},
  year      = {2019}
}